# -*- coding: utf-8 -*-
"""CVD_Capstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ga2GN9qxW9-tee9MrKHSnL5WTXcesDRA
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as mp
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from imblearn.over_sampling import RandomOverSampler
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.decomposition import PCA
import cvxopt
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils import resample
from collections import Counter

"""Data Set cleaning and Preprocessing"""

cols = ["age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thal", "target"]
df = pd.read_csv("processed.cleveland.data",  names=cols)
df['target'] = df['target'].replace({2: 1, 3: 1, 4: 1})
df.head()

"""Empty and unwanted data cleaning

"""

df = df.replace('?', np.nan)
for col in df.columns:
    df[col] = pd.to_numeric(df[col], errors='coerce')

df = df.fillna(df.mean())

for label in cols[:-1]:
   mp.hist(df[df['target'] == 1][label], color='blue', label='heart disease present', alpha=0.7, density=True)
   mp.hist(df[df['target'] == 0][label], color='green', label='heart disease absent', alpha=0.7, density=True)
   mp.title(label)
   mp.ylabel('Probability')
   mp.xlabel(label)
   mp.legend()
   mp.show()

"""Splitting the data set for training and testing (70:30)"""

train, test = np.split(df.sample(frac=1, random_state=42), [int(0.70*len(df))])

"""Data before oversampling"""

print(len(train [train['target'] == 1]))
print(len(train [train['target'] == 0]))

"""Scaling and oversampling

"""

def scale_data(df, oversample = False):
  x = df[df.columns[:-1]].values
  y = df[df.columns[-1]].values

  scaler = StandardScaler()
  x = scaler.fit_transform(x)
  RANDOM_STATE = 42;
  if oversample:
    ros = RandomOverSampler(random_state=RANDOM_STATE)
    x, y = ros.fit_resample(x, y)
  data = np.hstack((x, np.reshape(y, (-1, 1)))) #creating a 2D  numpy array

  return data, x, y

train_s, x_train, y_train = scale_data(train, oversample=True)
test_s, x_test, y_test = scale_data(test, oversample=False)

sum(y_train==1)

sum(y_train==0)

import math
math.sqrt(len(y_test))

"""K-Nearest N

"""

k_values = list(range(3, 31, 2))  #  only odd K values between 3 and 30
error_rates = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(x_train, y_train)
    y_pred = knn.predict(x_test)

    error = 1 - accuracy_score(y_test, y_pred)
    error_rates.append(error)

plt.figure(figsize=(8,6))
plt.plot(k_values, error_rates, marker='o', linestyle='dashed', color='b')
plt.xlabel("Number of Neighbors (K)")
plt.ylabel("Error Rate")
plt.title("Elbow Method for Optimal K Selection")
plt.grid()
plt.show()

kn_model = KNeighborsClassifier(n_neighbors=19, p =2, metric='euclidean')
kn_model.fit(x_train, y_train)

y_pred = kn_model.predict(x_test)

y_pred

y_test

knn_conf_matrix = confusion_matrix(y_test, y_pred)
knn_acc_score = accuracy_score(y_test, y_pred)
print("confusion matrix")
print(knn_conf_matrix)
print("\n")
print("Accuracy of K-NeighborsClassifier:",knn_acc_score*100,'\n')
print(classification_report(y_test, y_pred))

y_train_pred = knn.predict(x_train)
y_test_pred = knn.predict(x_test)


# TRAINING RESULTS

train_conf_matrix = confusion_matrix(y_train, y_train_pred)
train_acc_score = accuracy_score(y_train, y_train_pred)

print("TRAINING PERFORMANCE")
print("Confusion Matrix:")
print(train_conf_matrix)
print("Accuracy of K-NeighborsClassifier on Training Data:", train_acc_score * 100, '\n')
print("Classification Report:")
print(classification_report(y_train, y_train_pred))

# TESTING RESULTS

test_conf_matrix = confusion_matrix(y_test, y_test_pred)
test_acc_score = accuracy_score(y_test, y_test_pred)

print("\n TESTING PERFORMANCE")
print("Confusion Matrix:")
print(test_conf_matrix)
print("Accuracy of K-NeighborsClassifier on Test Data:", test_acc_score * 100, '\n')
print("Classification Report:")
print(classification_report(y_test, y_test_pred))

"""Logistic Regression

"""

lg_model = LogisticRegression(random_state=0)
lg_model = lg_model.fit(x_train, y_train)
lg_pred = lg_model.predict(x_test)
lg_conf_matrix = confusion_matrix(y_test, lg_pred)
lg_acc_score = accuracy_score(y_test, lg_pred)
print("confussion matrix")
print(lg_conf_matrix)
print("\n")
print("Accuracy of Logistic Regression:",lg_acc_score*100,'\n')
print(classification_report(y_test,lg_pred))

lg_train_pred = lg_model.predict(x_train)
lg_test_pred = lg_model.predict(x_test)

# TRAINING RESULTS
lg_train_conf_matrix = confusion_matrix(y_train, lg_train_pred)
lg_train_acc_score = accuracy_score(y_train, lg_train_pred)

print("TRAINING PERFORMANCE (Logistic Regression)")
print("Confusion Matrix:")
print(lg_train_conf_matrix)
print("Accuracy on Training Data:", lg_train_acc_score * 100, '\n')
print("Classification Report:")
print(classification_report(y_train, lg_train_pred))

# TESTING RESULTS
lg_test_conf_matrix = confusion_matrix(y_test, lg_test_pred)
lg_test_acc_score = accuracy_score(y_test, lg_test_pred)

print("\nTESTING PERFORMANCE (Logistic Regression)")
print("Confusion Matrix:")
print(lg_test_conf_matrix)
print("Accuracy on Test Data:", lg_test_acc_score * 100, '\n')
print("Classification Report:")
print(classification_report(y_test, lg_test_pred))

"""SVM

"""

from sklearn.model_selection import GridSearchCV
param_grid = {
    'C': [0.1, 1, 10, 100],  # Regularization parameter
    'gamma': [0.01, 0.1, 0.5, 1, 5, 10],  # Range of gamma values
    'kernel': ['rbf']
}

grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(x_train, y_train)

best_gamma = grid_search.best_params_['gamma']
best_C = grid_search.best_params_['C']
best_score = grid_search.best_score_

print(f"Best Gamma: {best_gamma}")
print(f"Best C: {best_C}")
print(f"Best Cross-Validation Accuracy: {best_score:.4f}")

svm_model = SVC(kernel='rbf', C=best_C, gamma=0.08)
svm_model.fit(x_train, y_train)

y_pred = svm_model.predict(x_test)
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy with Best Gamma and C: {test_accuracy:.4f}")

def calculate_variance(data):

    if isinstance(data, pd.DataFrame):
        return data.var(ddof=0)
    elif isinstance(data, np.ndarray):
        return np.var(data, axis=0)
    else:
        raise ValueError("Input should be a NumPy array or Pandas DataFrame.")
feature_variance = calculate_variance(x_train)

# Compute default gamma value
n_features = x_train.shape[1]
gamma_default = 1 / (n_features * feature_variance.mean())

print(f"Calculated Default Gamma: {gamma_default:.6f}")

# Grid Search for Best Parameters
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.01, 0.1, 0.5, 1, 5, 10, gamma_default],
    'kernel': ['rbf']
}

grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(x_train, y_train)

# Get the best gamma and C values
best_gamma = grid_search.best_params_['gamma']
best_C = grid_search.best_params_['C']
best_score = grid_search.best_score_

print(f"Best Gamma: {best_gamma}")
print(f"Best C: {best_C}")
print(f"Best Cross-Validation Accuracy: {best_score:.4f}")

svm_model = SVC(kernel='rbf', C=best_C, gamma=best_gamma)
svm_model.fit(x_train, y_train)

y_pred = svm_model.predict(x_test)
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy with Best Gamma and C: {test_accuracy:.4f}")

y_pred = svm_model.predict(x_test)
svm_conf_matrix = confusion_matrix(y_test, y_pred)
svm_acc_score = accuracy_score(y_test, y_pred)
print("confusion matrix")
print(svm_conf_matrix)
print("\n")
print("Accuracy of SVM:",svm_acc_score*100,'\n')
print(classification_report(y_test,y_pred))

svm_train_pred = svm_model.predict(x_train)

svm_test_pred = svm_model.predict(x_test)

svm_train_conf_matrix = confusion_matrix(y_train, svm_train_pred)
svm_train_acc_score = accuracy_score(y_train, svm_train_pred)

print("TRAINING PERFORMANCE (SVM)")
print("Confusion Matrix:")
print(svm_train_conf_matrix)
print("Accuracy on Training Data:", svm_train_acc_score * 100, '\n')
print("Classification Report:")
print(classification_report(y_train, svm_train_pred))

svm_test_conf_matrix = confusion_matrix(y_test, svm_test_pred)
svm_test_acc_score = accuracy_score(y_test, svm_test_pred)

print("\nTESTING PERFORMANCE (SVM)")
print("Confusion Matrix:")
print(svm_test_conf_matrix)
print("Accuracy on Test Data:", svm_test_acc_score * 100, '\n')
print("Classification Report:")
print(classification_report(y_test, svm_test_pred))

pca = PCA(n_components=2)
X_pca = pca.fit_transform(x_train)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train, cmap='coolwarm')
plt.title("PCA Visualization of Data Separability")
plt.show()

pca = PCA(n_components=2)
x_train_2d = pca.fit_transform(x_train)

svm_model = SVC(kernel='rbf', C=1, gamma='scale')
svm_model.fit(x_train_2d, y_train)

# Create meshgrid to plot decision boundary
x_min, x_max = x_train_2d[:, 0].min() - 1, x_train_2d[:, 0].max() + 1
y_min, y_max = x_train_2d[:, 1].min() - 1, x_train_2d[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

Z = svm_model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(x_train_2d[:, 0], x_train_2d[:, 1], c=y_train, cmap=plt.cm.bwr, edgecolors='k')
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("SVM Decision Boundary for Training Data")
plt.show()

X = np.array([
    [0.63, -1.38, -0.24],
    [0.94, 0.92, 1.50],
    [0.94, 0.92, -0.38],
    [-1.42, 0.15, 0.32],
    [-1.10, -0.62, -1.20]
])

Y = np.array([1, -1, 1, -1, 1])


C = 10
gamma = 0.1

def rbf_kernel(X1, X2, gamma):
    return np.exp(-gamma * np.linalg.norm(X1 - X2) ** 2)

n_samples = X.shape[0]
K = np.zeros((n_samples, n_samples))

for i in range(n_samples):
    for j in range(n_samples):
        K[i, j] = rbf_kernel(X[i], X[j], gamma)

P = cvxopt.matrix(np.outer(Y, Y) * K)
q = cvxopt.matrix(-np.ones(n_samples))
G = cvxopt.matrix(np.vstack((-np.eye(n_samples), np.eye(n_samples))))
h = cvxopt.matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * C)))
A = cvxopt.matrix(Y, (1, n_samples), 'd')
b = cvxopt.matrix(0.0)

solution = cvxopt.solvers.qp(P, q, G, h, A, b)

alphas = np.ravel(solution['x'])

print("Optimal Alpha values (only non-zero support vectors):")
for i in range(len(alphas)):
    if alphas[i] > 1e-5:
        print(f"α[{i}] = {alphas[i]:.4f}")
support_vector_idx = np.where(alphas > 1e-5)[0]
bias = np.mean([Y[i] - sum(alphas[j] * Y[j] * K[i, j] for j in support_vector_idx) for i in support_vector_idx])

print(f"Bias (b) = {bias:.4f}")

"""Decision Trees

"""

clf = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=42)
clf.fit(x_train, y_train)

plt.figure(figsize=(20, 10))
plot_tree(
    clf,
    feature_names=cols[:-1],
    class_names=["No Heart Disease", "Heart Disease"],
    filled=True,
    rounded=True
)
plt.title("Decision Tree Trained on X_train")
plt.show()

clf_gini = DecisionTreeClassifier(criterion = "gini", random_state = 42, max_depth=3, max_leaf_nodes=5)
clf_gini.fit(x_train, y_train)
y_pred = clf_gini.predict(x_test)
dsc_conf_matrix = confusion_matrix(y_test, y_pred)
dsc_acc_score = accuracy_score(y_test, y_pred)
print("confusion matrix")
print(dsc_conf_matrix)
print("\n")
print("Accuracy of Decision Tree:",dsc_acc_score*100,'\n')
print(classification_report(y_test,y_pred))

clf_gini = DecisionTreeClassifier(criterion="gini", random_state=42, max_depth=3, max_leaf_nodes=5)
clf_gini.fit(x_train, y_train)

dsc_train_pred = clf_gini.predict(x_train)
dsc_test_pred = clf_gini.predict(x_test)


# TRAINING RESULTS

dsc_train_conf_matrix = confusion_matrix(y_train, dsc_train_pred)
dsc_train_acc_score = accuracy_score(y_train, dsc_train_pred)

print("TRAINING PERFORMANCE (Decision Tree)")
print("Confusion Matrix:")
print(dsc_train_conf_matrix)
print("Accuracy on Training Data:", dsc_train_acc_score * 100, '\n')
print("Classification Report:")
print(classification_report(y_train, dsc_train_pred))


# TESTING RESULTS

dsc_test_conf_matrix = confusion_matrix(y_test, dsc_test_pred)
dsc_test_acc_score = accuracy_score(y_test, dsc_test_pred)

print("\nTESTING PERFORMANCE (Decision Tree)")
print("Confusion Matrix:")
print(dsc_test_conf_matrix)
print("Accuracy on Test Data:", dsc_test_acc_score * 100, '\n')
print("Classification Report:")
print(classification_report(y_test, dsc_test_pred))

clf_entropy = DecisionTreeClassifier(criterion = "entropy", random_state = 0, max_depth=3, max_leaf_nodes=5)
clf_entropy.fit(x_train, y_train)
y_pred = clf_entropy.predict(x_test)
dsc_conf_matrix = confusion_matrix(y_test, y_pred)
dsc_acc_score = accuracy_score(y_test, y_pred)
print("confusion matrix")
print(dsc_conf_matrix)
print("\n")
print("Accuracy of Decision Tree:",dsc_acc_score*100,'\n')
print(classification_report(y_test,y_pred))

"""Random Forest"""

randclass = RandomForestClassifier(n_estimators = 50, random_state=42, max_depth=15)
randclass.fit(x_train, y_train)
y_pred = randclass.predict(x_test)
rf_conf_matrix = confusion_matrix(y_test, y_pred)
rf_acc_score = accuracy_score(y_test, y_pred)
print("confusion matrix")
print(rf_conf_matrix)
print("\n")
print("Accuracy of Random Forest:",rf_acc_score*100,'\n')
print(classification_report(y_test,y_pred))

randclass = RandomForestClassifier(n_estimators=50, random_state=42, max_depth=15)
randclass.fit(x_train, y_train)


rf_train_pred = randclass.predict(x_train)
rf_test_pred = randclass.predict(x_test)

# TRAINING RESULTS
rf_train_conf_matrix = confusion_matrix(y_train, rf_train_pred)
rf_train_acc_score = accuracy_score(y_train, rf_train_pred)

print("TRAINING PERFORMANCE (Random Forest)")
print("Confusion Matrix:")
print(rf_train_conf_matrix)
print("Accuracy on Training Data:", rf_train_acc_score * 100, '\n')
print("Classification Report:")
print(classification_report(y_train, rf_train_pred))

# TESTING RESULTS
rf_test_conf_matrix = confusion_matrix(y_test, rf_test_pred)
rf_test_acc_score = accuracy_score(y_test, rf_test_pred)

print("\nTESTING PERFORMANCE (Random Forest)")
print("Confusion Matrix:")
print(rf_test_conf_matrix)
print("Accuracy on Test Data:", rf_test_acc_score * 100, '\n')
print("Classification Report:")
print(classification_report(y_test, rf_test_pred))

x_tr = df[train.columns[:-1]].values
y_tr = df[train.columns[-1]].values

x_tes = df[test.columns[:-1]].values
y_tes = df[test.columns[-1]].values

"""RF Raw data"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

randclass = RandomForestClassifier(n_estimators=50, max_depth=15, random_state=42)
randclass.fit(x_tr, y_tr)
y_pred = randclass.predict(x_tes)
rf_conf_matrix = confusion_matrix(y_tes, y_pred)
rf_acc_score = accuracy_score(y_tes, y_pred)
print("confusion matrix")
print(rf_conf_matrix)
print("\n")
print("Accuracy of Random Forest :",rf_acc_score*100,'\n')
print(classification_report(y_tes,y_pred))

# Sample Normalized Data
data = {
    'Age': [0.63, 0.94, 0.94, -1.42, -1.10],
    'CP': [-1.38, 0.92, 0.92, 0.15, -0.62],
    'Chol': [-0.24, 1.50, -0.38, 0.32, -1.20],
    'Target': [0, 1, 1, 0, 0]
}

df = pd.DataFrame(data)

X = df[['Age', 'CP', 'Chol']]
y = df['Target']

# New data point to predict
X_new = np.array([[-0.79, 0.15, -0.67]])

n_trees = 3
n_features_per_split = 2
predictions = []


for i in range(n_trees):
    # Bootstrap sample
    X_sample, y_sample = resample(X, y, replace=True, n_samples=len(X), random_state=i)

    # Random feature subset
    selected_features = np.random.choice(X.columns, n_features_per_split, replace=False)


    clf = DecisionTreeClassifier(random_state=i, max_depth=3)
    clf.fit(X_sample[selected_features], y_sample)


    pred = clf.predict(X_new[:, [X.columns.get_loc(f) for f in selected_features]])[0]
    predictions.append(pred)

    print(f"Tree {i+1} trained on features {list(selected_features)} → Prediction: {pred}")

# Final Prediction by Majority Voting
final_pred = Counter(predictions).most_common(1)[0][0]
print(f"\nFinal Prediction (Majority Vote): {final_pred} → {'Heart Disease' if final_pred == 1 else 'No Heart Disease'}")

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import plot_tree

# Sample dataset (same as earlier)
data = {
    'Age': [0.63, 0.94, 0.94, -1.42, -1.10],
    'CP': [-1.38, 0.92, 0.92, 0.15, -0.62],
    'Chol': [-0.24, 1.50, -0.38, 0.32, -1.20],
    'Target': [0, 1, 1, 0, 0]
}

df = pd.DataFrame(data)
X = df[['CP', 'Chol']]
y = df['Target']

# Train Random Forest with 3 trees
rf = RandomForestClassifier(n_estimators=2, random_state=42)
rf.fit(X, y)

# Plot each tree in the forest
for idx, tree in enumerate(rf.estimators_):
    plt.figure(figsize=(10, 6))
    plot_tree(
        tree,
        feature_names=X.columns,
        class_names=['No HD', 'HD'],
        filled=True,
        rounded=True
    )
    plt.title(f"Decision Tree {idx+1} from Random Forest")
    plt.show()

"""80:37 split

"""

train_s, x_train, y_train = scale_data(train, oversample=True)
print(train_s.shape)

# Reconstruct DataFrame from train_s
balanced_df = pd.DataFrame(train_s, columns=[f'feature_{i}' for i in range(train_s.shape[1]-1)] + ['target'])

# Split each class into 80/37
class_0 = balanced_df[balanced_df['target'] == 0]
class_1 = balanced_df[balanced_df['target'] == 1]

train_0, test_0 = train_test_split(class_0, test_size=37, random_state=42)
train_1, test_1 = train_test_split(class_1, test_size=37, random_state=42)

train_set = pd.concat([train_0, train_1]).sample(frac=1, random_state=42)
test_set = pd.concat([test_0, test_1]).sample(frac=1, random_state=42)

# Extract features and target
x_train = train_set.drop(columns='target').values
y_train = train_set['target'].values
x_test = test_set.drop(columns='target').values
y_test = test_set['target'].values

scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

def evaluate_model(model, x_train, y_train, x_test, y_test, model_name="Model"):
    model.fit(x_train, y_train)
    y_train_pred = model.predict(x_train)
    y_test_pred = model.predict(x_test)

    print(f"\nTRAINING PERFORMANCE ({model_name})")
    print("Confusion Matrix:\n", confusion_matrix(y_train, y_train_pred))
    print("Accuracy:", round(accuracy_score(y_train, y_train_pred) * 100, 2), "%")
    print("Classification Report:\n", classification_report(y_train, y_train_pred))

    print(f"\nTESTING PERFORMANCE ({model_name})")
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred))
    print("Accuracy:", round(accuracy_score(y_test, y_test_pred) * 100, 2), "%")
    print("Classification Report:\n", classification_report(y_test, y_test_pred))
    print("="*60)

# models
models = {
    "K-Nearest Neighbors": KNeighborsClassifier(n_neighbors=5),
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Support Vector Machine": SVC(kernel='rbf', probability=True, random_state=42),
    "Decision Tree": DecisionTreeClassifier(criterion="gini", max_depth=3, max_leaf_nodes=5, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=50, max_depth=15, random_state=42)
}

# Evaluating all models
for name, model in models.items():
    evaluate_model(model, x_train, y_train, x_test, y_test, model_name=name)

import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, plot_tree
import numpy as np

# Sample normalized data
X = np.array([
    [-1.42],
    [-1.10],
    [0.63],
    [0.94],
    [0.94],
])
y = np.array([0, 0, 0, 1, 1])

# Train decision tree
clf = DecisionTreeClassifier(criterion='gini', max_depth=1, random_state=42)
clf.fit(X, y)


plt.figure(figsize=(8, 6))
plot_tree(
    clf,
    feature_names=["CP (Z)"],
    class_names=["No HD", "HD"],
    filled=True,
    rounded=True,
    fontsize=12
)
plt.title("Decision Tree Trained on Sample Normalized Data", fontsize=14)
plt.tight_layout()
plt.show()

data = {
    'Age (Z)': [-1.42, -1.10, 0.63, 0.94, 0.94],
    'Label': [0, 0, 0, 1, 1]
}

df = pd.DataFrame(data)

X = df[['Age (Z)']]
y = df['Label']

clf = DecisionTreeClassifier(criterion='gini', max_depth=1, random_state=42)
clf.fit(X, y)

plt.figure(figsize=(8, 6))
plot_tree(
    clf,
    feature_names=['Age (Z)'],
    class_names=['No HD', 'HD'],
    filled=True,
    rounded=True,
    impurity=True,
    label='all'
)
plt.tight_layout()
plt.show()